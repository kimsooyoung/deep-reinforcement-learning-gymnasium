{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPesmxBfkpQfZiBBsZntiu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimsooyoung/deep-reinforcement-learning-gymnasium/blob/main/lab-taxi/Taxi_Problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taxi Problem\n",
        "\n",
        "### Getting Started\n",
        "\n",
        "Read the description of the environment in subsection 3.1 of [this paper](https://arxiv.org/pdf/cs/9905014.pdf).  You can verify that the description in the paper matches the OpenAI Gym environment by peeking at the code [here](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py)."
      ],
      "metadata": {
        "id": "uEOLrcFzfB51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions\n",
        "\n",
        "The repository contains three files:\n",
        "- `agent.py`: Develop your reinforcement learning agent here.  This is the only file that you should modify.\n",
        "- `monitor.py`: The `interact` function tests how well your agent learns from interaction with the environment.\n",
        "- `main.py`: Run this file in the terminal to check the performance of your agent."
      ],
      "metadata": {
        "id": "tgBXUB5RfHFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run `main.py`, the agent that you specify in `agent.py` interacts with the environment for 20,000 episodes.  The details of the interaction are specified in `monitor.py`, which returns two variables: `avg_rewards` and `best_avg_reward`.\n",
        "- `avg_rewards` is a deque where `avg_rewards[i]` is the average (undiscounted) return collected by the agent from episodes `i+1` to episode `i+100`, inclusive.  So, for instance, `avg_rewards[0]` is the average return collected by the agent over the first 100 episodes.\n",
        "- `best_avg_reward` is the largest entry in `avg_rewards`.  This is the final score that you should use when determining how well your agent performed in the task.\n",
        "\n",
        "Your assignment is to modify the `agents.py` file to improve the agent's performance.\n",
        "- Use the `__init__()` method to define any needed instance variables.  Currently, we define the number of actions available to the agent (`nA`) and initialize the action values (`Q`) to an empty dictionary of arrays.  Feel free to add more instance variables; for example, you may find it useful to define the value of epsilon if the agent uses an epsilon-greedy policy for selecting actions.\n",
        "- The `select_action()` method accepts the environment state as input and returns the agent's choice of action.  The default code that we have provided randomly selects an action.\n",
        "- The `step()` method accepts a (`state`, `action`, `reward`, `next_state`) tuple as input, along with the `done` variable, which is `True` if the episode has ended.  The default code (which you should certainly change!) increments the action value of the previous state-action pair by 1.  You should change this method to use the sampled tuple of experience to update the agent's knowledge of the problem.\n"
      ],
      "metadata": {
        "id": "FSaSOVqNfIb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Once you have modified the function, you need only run `python main.py` to test your new agent.\n",
        "\n",
        "### **OpenAI Gym [defines \"solving\"](https://gym.openai.com/envs/Taxi-v1/) this task as getting average return of 9.7 over 100 consecutive trials.**"
      ],
      "metadata": {
        "id": "iRCiYzvXfP_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4BJ2CHrgfUL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532e6a8d-d673-46b7-d7f7-955519b8f654"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.7.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque, defaultdict\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import math\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Tlt5_1S3gOrm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's explore Taxi Env\n",
        "\n",
        "* reference link : [gym/taxi](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py)"
      ],
      "metadata": {
        "id": "YSsscTechGAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "state, prob = env.reset()\n",
        "state, prob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoGkIUwxhV_o",
        "outputId": "954d57ed-39a6-4bf9-8783-bd554b9b4fb3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(242, {'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_policy = np.random.choice(6)\n",
        "next_state, reward, done, _, prob = env.step(random_policy)\n",
        "next_state, reward, done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CcpLnz6jisS",
        "outputId": "36bcbf25-c7c0-4a59-a4ef-7a7d41a063fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(142, -1, False)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Interact** Function from `monitor.py`\n",
        "\n"
      ],
      "metadata": {
        "id": "jcfHaGXYfy-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\" Monitor agent's performance.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of OpenAI Gym's Taxi-v1 environment\n",
        "    - agent: instance of class Agent (see Agent.py for details)\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "        # begin the episode\n",
        "        state, prob = env.reset()\n",
        "        # initialize the sampled reward\n",
        "        samp_reward = 0\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state, i_episode)\n",
        "            # agent performs the selected action\n",
        "            next_state, reward, done, _, prob = env.step(action)\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state (s <- s') to next time step\n",
        "            state = next_state\n",
        "            if done:\n",
        "                # save final sampled reward\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "        if (i_episode >= 100):\n",
        "            # get average reward from last 100 episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            # append to deque\n",
        "            avg_rewards.append(avg_reward)\n",
        "            # update best average reward\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "        # monitor progress\n",
        "        print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        # check if task is solved (according to OpenAI Gym)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print('\\nEnvironment solved in {} episodes.'.format(i_episode), end=\"\")\n",
        "            break\n",
        "        if i_episode == num_episodes: print('\\n')\n",
        "    return avg_rewards, best_avg_reward"
      ],
      "metadata": {
        "id": "dwpWUvEsfr-F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent class from `agent.py`"
      ],
      "metadata": {
        "id": "Bcn1f7RHf7o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\" Initialize agent.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        self.gamma = 1.0\n",
        "        self.alpha = 0.2\n",
        "\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "    def greedy_policy_prob(self, state, epsilon):\n",
        "\n",
        "        Q_cur = self.Q[state]\n",
        "        A_s = len(Q_cur)\n",
        "        output = np.ones(A_s) * epsilon / A_s\n",
        "        best_action = np.argmax(Q_cur)\n",
        "        output[best_action] = 1 - epsilon + epsilon / A_s\n",
        "        return output\n",
        "\n",
        "    def select_action(self, state, i_episode):\n",
        "        \"\"\" Given the state, select an action.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "\n",
        "        epsilon = 1.0 / i_episode\n",
        "        # epsilon = 0\n",
        "\n",
        "        greedy_prob = self.greedy_policy_prob(state, epsilon)\n",
        "        return np.random.choice(np.arange(self.nA), p=greedy_prob)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple. 1\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "\n",
        "        cur_Q = self.Q[state][action]\n",
        "\n",
        "        # update Q table\n",
        "        if done:\n",
        "            self.Q[state][action] = cur_Q + self.alpha * (reward - cur_Q)\n",
        "        else:\n",
        "            # S_1, A_1\n",
        "            next_Q = np.max(self.Q[next_state])\n",
        "            self.Q[state][action] = cur_Q + self.alpha * (reward + self.gamma * next_Q - cur_Q)"
      ],
      "metadata": {
        "id": "ZmHnzHLzgGfM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, execute **main** code."
      ],
      "metadata": {
        "id": "v8WnEr6wgsCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "agent = Agent()\n",
        "avg_rewards, best_avg_reward = interact(env, agent, 20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--bjhL8Eg0rK",
        "outputId": "633035f1-698f-4bd5-e56e-22bacca37983"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20000/20000 || Best average reward 9.26\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qG-UVHlg6Dk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}